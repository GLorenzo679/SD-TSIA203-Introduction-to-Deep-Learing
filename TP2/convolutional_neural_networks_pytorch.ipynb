{"cells":[{"cell_type":"markdown","metadata":{"id":"kaht-FPA1Jvq"},"source":["# Introduction\n","\n","## Lab2: Train a Convolutional Neural Network (CNN).\n","\n","In this Lab session we will learn how to train a CNN from scratch for classifying MNIST digits.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"UvxtTYHlVfRK"},"outputs":[],"source":["# import necessary libraries\n","import torch\n","import torchvision\n","from torchvision import transforms as T\n","import torch.nn.functional as F\n","import sklearn"]},{"cell_type":"markdown","metadata":{"id":"HYCvhGxKWyN7"},"source":["### Define LeNet\n","\n","![network architecture](https://www.researchgate.net/profile/Lucijano-Berus/publication/329891470/figure/fig1/AS:707347647307776@1545656229128/Architecture-of-LeNet-5-a-Convolutional-Neural-Network-for-digits-digits-recognition-An.ppm)\n","\n","Here we are going to define our first CNN which is **LeNet** in this case. This architecture has been introduced and is detailed in [this article](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf). To construct a LeNet we will be using some convolutional layers followed by some fully-connected layers. The convolutional layers can be simply defined using `torch.nn.Conv2d` module of `torch.nn` package. Details can be found [here](https://pytorch.org/docs/stable/nn.html#conv2d). Moreover, we will use pooling operation to reduce the size of convolutional feature maps. For this case we are going to use `torch.nn.functional.max_pool2d`. Details about maxpooling can be found [here](https://pytorch.org/docs/stable/nn.html#max-pool2d)\n","\n","Differently from our previous Lab, we will use a Rectified Linear Units (ReLU) as activation function with the help of `torch.nn.functional.relu`, replacing `torch.nn.Sigmoid`. Details about ReLU can be found [here](https://pytorch.org/docs/stable/nn.html#id26).\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"dMC_LDYdWkI7"},"outputs":[],"source":["class LeNet(torch.nn.Module):\n","    def __init__(self):\n","        super(LeNet, self).__init__()\n","\n","        # input channel = ?, output channels = ?, kernel size = ?\n","        # input image size = (?, ?), image output size = (?, ?)\n","        self.conv1 = torch.nn.Conv2d(1, 6, 5, padding=2)\n","\n","        # input channel = ?, output channels = ?, kernel size = ?\n","        # input image size = (?, ?), output image size = (?, ?)\n","        self.conv2 = torch.nn.Conv2d(6, 16, 5)\n","\n","        # input dim = ? ( H x W x C), output dim = ?\n","        self.fc1 = torch.nn.Linear(16 * 5 * 5, 120)\n","\n","        # input dim = ?, output dim = ?\n","        self.fc2 = torch.nn.Linear(120, 84)\n","\n","        # input dim = ?, output dim = ?\n","        self.fc3 = torch.nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        # Max Pooling with kernel size = ?\n","        # output size = (?, ?)\n","        x = F.max_pool2d(x, 2)\n","\n","        x = F.relu(self.conv2(x))\n","        # Max Pooling with kernel size = ?\n","        # output size = (?, ?)\n","        x = F.max_pool2d(x, 2)\n","\n","        # flatten the feature maps into a long vector\n","        x = x.view(x.shape[0], -1)\n","\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"gChf6TvWonrV"},"source":["### Define cost function\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"6j5UrBH3oek8"},"outputs":[],"source":["def get_cost_function():\n","    cost_function = torch.nn.CrossEntropyLoss()\n","    return cost_function"]},{"cell_type":"markdown","metadata":{"id":"U2TjXeVdorV9"},"source":["### Define the optimizer\n","\n","We will use SGD with learning rate-lr, weight_decay=wd and momentum=momentum\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"hBZN-WPboulR"},"outputs":[],"source":["def get_optimizer(net, lr, wd, momentum):\n","    optimizer = torch.optim.SGD(\n","        net.parameters(), lr=lr, weight_decay=wd, momentum=momentum\n","    )\n","    return optimizer"]},{"cell_type":"markdown","metadata":{"id":"wTkfrV64oxIL"},"source":["### Train and test functions\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"t-sE5vFio0lf"},"outputs":[],"source":["def test(net, data_loader, cost_function, device=\"cuda:0\"):\n","    samples = 0.0\n","    cumulative_loss = 0.0\n","    cumulative_accuracy = 0.0\n","\n","    net.eval()  # Strictly needed if network contains layers which has different behaviours between train and test\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(data_loader):\n","            # Load data into GPU\n","            inputs = inputs.to(device)\n","            targets = targets.to(device)\n","\n","            # Forward pass\n","            outputs = net(inputs)\n","\n","            # Apply the loss\n","            loss = cost_function(outputs, targets)\n","\n","            # Better print something\n","            samples += inputs.shape[0]\n","            cumulative_loss += (\n","                loss.item()\n","            )  # Note: the .item() is needed to extract scalars from tensors\n","            _, predicted = outputs.max(1)\n","            cumulative_accuracy += predicted.eq(targets).sum().item()\n","\n","    return cumulative_loss / samples, cumulative_accuracy / samples * 100\n","\n","\n","def train(net, data_loader, optimizer, cost_function, device=\"cuda:0\"):\n","    samples = 0.0\n","    cumulative_loss = 0.0\n","    cumulative_accuracy = 0.0\n","\n","    net.train()  # Strictly needed if network contains layers which has different behaviours between train and test\n","    for batch_idx, (inputs, targets) in enumerate(data_loader):\n","        # Load data into GPU\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","\n","        # Forward pass\n","        outputs = net(inputs)\n","\n","        # Apply the loss\n","        loss = cost_function(outputs, targets)\n","\n","        # Reset the optimizer\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Update parameters\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        # Better print something, no?\n","        samples += inputs.shape[0]\n","        cumulative_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        cumulative_accuracy += predicted.eq(targets).sum().item()\n","\n","    return cumulative_loss / samples, cumulative_accuracy / samples * 100"]},{"cell_type":"markdown","metadata":{"id":"T6IT0Lsgo8AM"},"source":["### Define the function that fetches a data loader that is then used during iterative training.\n","\n","We will learn a new thing in this function as how to Normalize the inputs given to the network.\n","\n","**_Why Normalization is needed_**?\n","\n","To have nice and stable training of the network it is recommended to normalize the network inputs between \\[-1, 1\\].\n","\n","**_How it can be done_**?\n","\n","This can be simply done using `torchvision.transforms.Normalize()` transform. Details can be found [here](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize).\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"qDxpo6uVo_8k"},"outputs":[],"source":["def get_data(batch_size, test_batch_size=256):\n","    # Prepare data transformations and then combine them sequentially\n","    transform = list()\n","    transform.append(T.ToTensor())  # converts Numpy to Pytorch Tensor\n","    transform.append(\n","        T.Normalize(mean=[0.5], std=[0.5])\n","    )  # Normalizes the Tensors between [-1, 1]\n","    transform = T.Compose(transform)  # Composes the above transformations into one.\n","\n","    # Load data\n","    full_training_data = torchvision.datasets.MNIST(\n","        \"./data\", train=True, transform=transform, download=True\n","    )\n","    test_data = torchvision.datasets.MNIST(\n","        \"./data\", train=False, transform=transform, download=True\n","    )\n","\n","    # Create train and validation splits\n","    num_samples = len(full_training_data)\n","    training_samples = int(num_samples * 0.5 + 1)\n","    validation_samples = num_samples - training_samples\n","\n","    training_data, validation_data = torch.utils.data.random_split(\n","        full_training_data, [training_samples, validation_samples]\n","    )\n","\n","    # Initialize dataloaders\n","    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n","    val_loader = torch.utils.data.DataLoader(\n","        validation_data, test_batch_size, shuffle=False\n","    )\n","    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n","\n","    return train_loader, val_loader, test_loader"]},{"cell_type":"markdown","metadata":{"id":"OHcB8f0AsY4n"},"source":["### Wrapping everything up\n","\n","Finally, we need a main function which initializes everything + the needed hyperparameters and loops over multiple epochs (printing the results).\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ip_R-hruse0Q"},"outputs":[],"source":["\"\"\"\n","Input arguments\n","  batch_size: Size of a mini-batch\n","  device: GPU where you want to train your network\n","  weight_decay: Weight decay co-efficient for regularization of weights\n","  momentum: Momentum for SGD optimizer\n","  epochs: Number of epochs for training the network\n","\"\"\"\n","\n","\n","def main(\n","    batch_size=128,\n","    device=\"cuda:0\",\n","    learning_rate=0.01,\n","    weight_decay=0.000001,\n","    momentum=0.9,\n","    epochs=50,\n","):\n","    device = torch.device(device)\n","    train_loader, val_loader, test_loader = get_data(batch_size)\n","\n","    # TODO for defining LeNet-5\n","    net = LeNet().to(device)\n","    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n","    cost_function = get_cost_function()\n","\n","    print(\"Before training:\")\n","    train_loss, train_accuracy = test(net, train_loader, cost_function)\n","    val_loss, val_accuracy = test(net, val_loader, cost_function)\n","    test_loss, test_accuracy = test(net, test_loader, cost_function)\n","\n","    print(\n","        \"\\t Training loss {:.5f}, Training accuracy {:.2f}\".format(\n","            train_loss, train_accuracy\n","        )\n","    )\n","    print(\n","        \"\\t Validation loss {:.5f}, Validation accuracy {:.2f}\".format(\n","            val_loss, val_accuracy\n","        )\n","    )\n","    print(\"\\t Test loss {:.5f}, Test accuracy {:.2f}\".format(test_loss, test_accuracy))\n","    print(\"-----------------------------------------------------\")\n","\n","    for e in range(epochs):\n","        train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n","        val_loss, val_accuracy = test(net, val_loader, cost_function)\n","        print(\"Epoch: {:d}\".format(e + 1))\n","        print(\n","            \"\\t Training loss {:.5f}, Training accuracy {:.2f}\".format(\n","                train_loss, train_accuracy\n","            )\n","        )\n","        print(\n","            \"\\t Validation loss {:.5f}, Validation accuracy {:.2f}\".format(\n","                val_loss, val_accuracy\n","            )\n","        )\n","        print(\"-----------------------------------------------------\")\n","\n","    print(\"After training:\")\n","    train_loss, train_accuracy = test(net, train_loader, cost_function)\n","    val_loss, val_accuracy = test(net, val_loader, cost_function)\n","    test_loss, test_accuracy = test(net, test_loader, cost_function)\n","\n","    print(\n","        \"\\t Training loss {:.5f}, Training accuracy {:.2f}\".format(\n","            train_loss, train_accuracy\n","        )\n","    )\n","    print(\n","        \"\\t Validation loss {:.5f}, Validation accuracy {:.2f}\".format(\n","            val_loss, val_accuracy\n","        )\n","    )\n","    print(\"\\t Test loss {:.5f}, Test accuracy {:.2f}\".format(test_loss, test_accuracy))\n","    print(\"-----------------------------------------------------\")"]},{"cell_type":"markdown","metadata":{"id":"ltdCMiB3t18h"},"source":["Lets train!\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"6d-z20H4tziL"},"outputs":[{"name":"stdout","output_type":"stream","text":["Before training:\n","\t Training loss 0.01807, Training accuracy 8.76\n","\t Validation loss 0.00907, Validation accuracy 8.74\n","\t Test loss 0.00923, Test accuracy 8.64\n","-----------------------------------------------------\n","Epoch: 1\n","\t Training loss 0.00938, Training accuracy 59.16\n","\t Validation loss 0.00109, Validation accuracy 91.66\n","-----------------------------------------------------\n","Epoch: 2\n","\t Training loss 0.00154, Training accuracy 93.80\n","\t Validation loss 0.00067, Validation accuracy 94.74\n","-----------------------------------------------------\n","Epoch: 3\n","\t Training loss 0.00098, Training accuracy 96.05\n","\t Validation loss 0.00048, Validation accuracy 96.16\n","-----------------------------------------------------\n","Epoch: 4\n","\t Training loss 0.00072, Training accuracy 97.14\n","\t Validation loss 0.00035, Validation accuracy 97.35\n","-----------------------------------------------------\n","Epoch: 5\n","\t Training loss 0.00057, Training accuracy 97.78\n","\t Validation loss 0.00033, Validation accuracy 97.38\n","-----------------------------------------------------\n","Epoch: 6\n","\t Training loss 0.00046, Training accuracy 98.15\n","\t Validation loss 0.00030, Validation accuracy 97.65\n","-----------------------------------------------------\n","Epoch: 7\n","\t Training loss 0.00039, Training accuracy 98.42\n","\t Validation loss 0.00031, Validation accuracy 97.68\n","-----------------------------------------------------\n","Epoch: 8\n","\t Training loss 0.00035, Training accuracy 98.56\n","\t Validation loss 0.00026, Validation accuracy 97.97\n","-----------------------------------------------------\n","Epoch: 9\n","\t Training loss 0.00029, Training accuracy 98.79\n","\t Validation loss 0.00023, Validation accuracy 98.24\n","-----------------------------------------------------\n","Epoch: 10\n","\t Training loss 0.00026, Training accuracy 98.93\n","\t Validation loss 0.00022, Validation accuracy 98.29\n","-----------------------------------------------------\n","Epoch: 11\n","\t Training loss 0.00022, Training accuracy 99.03\n","\t Validation loss 0.00022, Validation accuracy 98.26\n","-----------------------------------------------------\n","Epoch: 12\n","\t Training loss 0.00018, Training accuracy 99.27\n","\t Validation loss 0.00026, Validation accuracy 98.07\n","-----------------------------------------------------\n","Epoch: 13\n","\t Training loss 0.00017, Training accuracy 99.25\n","\t Validation loss 0.00021, Validation accuracy 98.42\n","-----------------------------------------------------\n","Epoch: 14\n","\t Training loss 0.00014, Training accuracy 99.42\n","\t Validation loss 0.00022, Validation accuracy 98.40\n","-----------------------------------------------------\n","Epoch: 15\n","\t Training loss 0.00012, Training accuracy 99.53\n","\t Validation loss 0.00022, Validation accuracy 98.43\n","-----------------------------------------------------\n","Epoch: 16\n","\t Training loss 0.00011, Training accuracy 99.53\n","\t Validation loss 0.00022, Validation accuracy 98.46\n","-----------------------------------------------------\n","Epoch: 17\n","\t Training loss 0.00011, Training accuracy 99.54\n","\t Validation loss 0.00023, Validation accuracy 98.38\n","-----------------------------------------------------\n","Epoch: 18\n","\t Training loss 0.00009, Training accuracy 99.68\n","\t Validation loss 0.00022, Validation accuracy 98.53\n","-----------------------------------------------------\n","Epoch: 19\n","\t Training loss 0.00007, Training accuracy 99.75\n","\t Validation loss 0.00022, Validation accuracy 98.54\n","-----------------------------------------------------\n","Epoch: 20\n","\t Training loss 0.00006, Training accuracy 99.80\n","\t Validation loss 0.00025, Validation accuracy 98.46\n","-----------------------------------------------------\n","Epoch: 21\n","\t Training loss 0.00006, Training accuracy 99.81\n","\t Validation loss 0.00023, Validation accuracy 98.53\n","-----------------------------------------------------\n","Epoch: 22\n","\t Training loss 0.00005, Training accuracy 99.85\n","\t Validation loss 0.00023, Validation accuracy 98.60\n","-----------------------------------------------------\n","Epoch: 23\n","\t Training loss 0.00004, Training accuracy 99.87\n","\t Validation loss 0.00026, Validation accuracy 98.50\n","-----------------------------------------------------\n","Epoch: 24\n","\t Training loss 0.00005, Training accuracy 99.80\n","\t Validation loss 0.00027, Validation accuracy 98.36\n","-----------------------------------------------------\n","Epoch: 25\n","\t Training loss 0.00003, Training accuracy 99.90\n","\t Validation loss 0.00025, Validation accuracy 98.52\n","-----------------------------------------------------\n","Epoch: 26\n","\t Training loss 0.00002, Training accuracy 99.94\n","\t Validation loss 0.00033, Validation accuracy 98.25\n","-----------------------------------------------------\n","Epoch: 27\n","\t Training loss 0.00004, Training accuracy 99.82\n","\t Validation loss 0.00028, Validation accuracy 98.45\n","-----------------------------------------------------\n","Epoch: 28\n","\t Training loss 0.00004, Training accuracy 99.85\n","\t Validation loss 0.00025, Validation accuracy 98.62\n","-----------------------------------------------------\n","Epoch: 29\n","\t Training loss 0.00002, Training accuracy 99.96\n","\t Validation loss 0.00024, Validation accuracy 98.70\n","-----------------------------------------------------\n","Epoch: 30\n","\t Training loss 0.00001, Training accuracy 99.96\n","\t Validation loss 0.00024, Validation accuracy 98.67\n","-----------------------------------------------------\n","Epoch: 31\n","\t Training loss 0.00001, Training accuracy 99.99\n","\t Validation loss 0.00025, Validation accuracy 98.69\n","-----------------------------------------------------\n","Epoch: 32\n","\t Training loss 0.00001, Training accuracy 100.00\n","\t Validation loss 0.00025, Validation accuracy 98.68\n","-----------------------------------------------------\n","Epoch: 33\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00025, Validation accuracy 98.70\n","-----------------------------------------------------\n","Epoch: 34\n","\t Training loss 0.00000, Training accuracy 99.99\n","\t Validation loss 0.00026, Validation accuracy 98.72\n","-----------------------------------------------------\n","Epoch: 35\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00026, Validation accuracy 98.72\n","-----------------------------------------------------\n","Epoch: 36\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00027, Validation accuracy 98.70\n","-----------------------------------------------------\n","Epoch: 37\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00027, Validation accuracy 98.72\n","-----------------------------------------------------\n","Epoch: 38\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00027, Validation accuracy 98.69\n","-----------------------------------------------------\n","Epoch: 39\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00027, Validation accuracy 98.73\n","-----------------------------------------------------\n","Epoch: 40\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00027, Validation accuracy 98.71\n","-----------------------------------------------------\n","Epoch: 41\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00027, Validation accuracy 98.72\n","-----------------------------------------------------\n","Epoch: 42\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00027, Validation accuracy 98.72\n","-----------------------------------------------------\n","Epoch: 43\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00028, Validation accuracy 98.71\n","-----------------------------------------------------\n","Epoch: 44\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00028, Validation accuracy 98.73\n","-----------------------------------------------------\n","Epoch: 45\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00028, Validation accuracy 98.70\n","-----------------------------------------------------\n","Epoch: 46\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00028, Validation accuracy 98.70\n","-----------------------------------------------------\n","Epoch: 47\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00028, Validation accuracy 98.70\n","-----------------------------------------------------\n","Epoch: 48\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00028, Validation accuracy 98.71\n","-----------------------------------------------------\n","Epoch: 49\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00029, Validation accuracy 98.70\n","-----------------------------------------------------\n","Epoch: 50\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00029, Validation accuracy 98.71\n","-----------------------------------------------------\n","After training:\n","\t Training loss 0.00000, Training accuracy 100.00\n","\t Validation loss 0.00029, Validation accuracy 98.71\n","\t Test loss 0.00022, Test accuracy 98.79\n","-----------------------------------------------------\n"]}],"source":["main()"]},{"cell_type":"markdown","metadata":{"id":"NQBDT48CKMVC"},"source":["Using the proper metric from sklearn, check which character is most frequently confused with which: can you explain why ?\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"sm5b4MV1KzQ7"},"outputs":[],"source":["# Using the proper metric from sklearn, check which character is most frequently confused with which: can you explain why ?"]},{"cell_type":"markdown","metadata":{"id":"ZOvixkfeMHrD"},"source":["The LeNet5 architecture can also be implemented using the sequential API ([see documentation ](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)). Reimplement it with this API.\n"]},{"cell_type":"markdown","metadata":{"id":"7i3-pC5xAyu5"},"source":["## Experiments\n","\n","- Implement adaptive early stopping: if the validation loss did not decrease for K consecutive epochs, stop training.\n","- Change dataset in order to evaluate the LeNet5 network on cifar10 dataset. You can have a look at the pytorch to easily access the cifar10 dataset.\n","- Try to improve performance with:\n","  - data-augmentation\n","  - dropout\n","- Implement the resnet18 architecture using the Resnet18 class from pytorch.\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1o6orxhxIapGZWt5FD4v7T8BRwdcSCEW4","timestamp":1684744697778},{"file_id":"1dpY2_Um7w8Qqx5c0e_vD2qaXXFW2LE3j","timestamp":1684248464934}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
